# Debugging Playbook

| Phase                             | Step                                 | Priority | Tag           | Remarks                                                                                                                                                                      |
|-----------------------------------|--------------------------------------|----------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1 – Initial Diagnosis**         | Check logs, APM in affected duration | High     | Diagnostics   | Check logs, APM metrics in the affected duration, to understand the issue.                                                                                                   |
|                                   | Check recent changes                 | High     | Diagnostics   | Review recent deployments, dependency upgrades, infrastructure changes, and configuration edits that might have triggered the issue.                                         |
|                                   | Validate dependencies                | High     | Diagnostics   | Verify the health and responsiveness of databases, caches, message queues, and external APIs.<br/>- Many incidents are caused by downstream failures.                        |
|                                   | Monitor key metrics                  | High     | Diagnostics   | Compare CPU, memory, disk, network, and service-specific metrics against normal baselines.<br/>- Look for unusual spikes, drops, or saturation.                              |
| **2 – Mitigation**                | Rollback to last stable version      | High     | Mitigation    | In case of live production issue which would take time to resolve, it’s better to revert to last stable version on live quickly.                                             |
|                                   | Feature flags / partial rollbacks    | High     | Mitigation    | If available, use feature toggles or [canary rollbacks](DeploymentTechniques.md) to isolate faulty code paths without a full system rollback.                                |
| **3 – Root Cause Investigation**  | Reproduce the issue                  | High     | Diagnostics   | Attempt to recreate the bug in staging or local with the same configs, data, and load profile.<br/>- This helps confirm the cause and scope before touching production.      |
|                                   | Error tracing & correlation          | Medium   | Diagnostics   | Use [distributed tracing tools](3_Observability) (e.g. OpenTelemetry, Jaeger, Zipkin) to follow failing requests through multiple services.                                  |
| **4 – Prevention & Optimization** | Ensure observability                 | Medium   | Observability | Setting proper [observability](3_Observability) is necessary to debug production issues, especially in distributed systems.                                                  |
|                                   | Ensure structured logging            | Medium   | Observability | Use [structured logging](3_Observability/StructuredLogging.md) during development to enable observability and debug issues.                                                  |
|                                   | Ensure profiling                     | Medium   | Observability | Ensure [profiling](https://github.com/Anshul619/Golang/blob/main/Debugging-Profiling/Readme.md) is enabled in Go (or relevant language) to identify performance bottlenecks. |
|                                   | API performance tuning principles    | Medium   | Optimization  | [Read more](https://github.com/Anshul619/HLD-System-Designs/blob/main/PerformancePlaybook.md) on optimizing APIs and backend services for better performance and stability.  |
